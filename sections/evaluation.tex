\section{Evaluation}

We propose a two phase evaluation: first, we run our fuzzer at scale, on all functions in many R packages, to see how many successful call signatures are uncovered.
This large-scale study is conducted on \AT{a subset of?} the packages analyzed in previous work on inferring types for R functions~\cite{turcotte2020designing}, and results from fuzzing are compared with the results obtained by simply observing calls.
Then, we conduct case studies on \numFnsCaseStudy functions, and manually analyze the results.

\AT{Preamble.}
We pose and answer the following research questions:

\begin{enumerate}
    \item How many successful call signatures are discovered with fuzzing vs. by simply observing calls?
    \AT{This should give us a sense of the \textit{scale} of the problem.}
    \AT{I.e., compare the types we get from building the DB to the types from the fuzzer.}
    
    % \item To what degree does expanding the set of successful call signatures improve coverage?
    % \AT{This establishes the usefulness of looking specifically for new call signatures w.r.t. improving coverage, the usual metric for fuzzing papers.}
    % \item For those functions where fuzzing does not expand code coverage, do the coverage-guided techniques improve coverage and/or discover more signatures?
    % \AT{Hopefully, this shows that our efforts to go beyond basic random value selection are fruitful.}
    % \item Are the expanded type signatures useful? \AT{Just a thought; not sure how to measure this. Maybe move to discussion.}
    % \item how quickly is it to reach the same/ same number of type signatures by observing calls vs fuzzing, vs fuzzing + coverage-guided? \PB{Related to 3) ?} \AT{I would argue we can phrase this as an initial RQ to determine optimal fuzzer configuration.}
    % \item How many bugs are found? \AT{My hope is that we can have a section titled ``...and all the bugs we found along the way!' at the end of the evaluation :p}
    % \item What is the ideal database size?
    % \item Some traces were obtained with arguments that were much larger or more complex than they needed to be. Can we find equivalent, ``smaller'' calls to give to users? \AT{I don't think this would be too hard, we could either have heuristics or just try to sample smaller values? Open to ideas.}
\end{enumerate}

\paragraph{Experiment Server} 
We ran all of our experiments on a \AT{prl3 server specs}.
All reported timing information is \AT{averaged over X runs, with standard deviations reported; timed experiments were conducted on a quiet server with few other processes running to minimize interference.}

%
% RQ1
%

\subsection{How many successful call signatures are discovered with fuzzing vs. by simply observing calls?}

\paragraph{Experimental Design}
\begin{itemize}
    \item We ran \tool on the \TODO{Y} \AT{exported?} functions from \TODO{X} packages.
    \item Recall the general approach of \tool: for a given package function $f$, we first consult the database for the pre-existing calls to $f$, and take the arguments of those calls as seeds for the fuzzer; then, our test generator iteratively generates new inputs by querying the database for values based on the initial seed.
    \item We compare the signatures generated by this process to the signatures corresponding to the pre-existing calls.
\end{itemize}

\paragraph{Results}
\AT{RQ1 results here...}
